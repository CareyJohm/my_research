{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "import pyfits, mechanize, bz2, os\n",
    "from matplotlib import pyplot as pl\n",
    "from numpy import *\n",
    "from scipy.optimize import leastsq\n",
    "import astropy.stats as sta\n",
    "from scipy import ndimage\n",
    "from glob import glob\n",
    "import pandas as pd  #為了讀csv檔。csv檔是以逗點分割的檔案。\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from astropy.io import ascii\n",
    "import concurrent.futures\n",
    "from time import sleep\n",
    "from astropy.table import Table\n",
    "import math as m\n",
    "import gzip\n",
    "import patoolib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('url.txt', 'r')\n",
    "x = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://dr2.lamost.org/sas/fits/HD110609N070823M01/spec-56270-HD110609N070823M01_sp07-075.fits.gz'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[5][0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lamo'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[5][0:-1][11:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for a in range(len(x[0][0:-1])):\n",
    "    if x[0][0:-1][a:a+4] == 'spec':\n",
    "        filename = x[0][0:-1][a:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spec-56270-HD110609N070823M01_sp07-075.fits.gz'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spec-56270-HD110609N070823M01_sp07-075.fits'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    \n",
    "    url_lamost = x[i][0:-1]\n",
    "\n",
    "    data_filename = url_lamost.split('/')[-1]\n",
    "    \n",
    "    \n",
    "    for a in range(len(x[i][0:-1])):\n",
    "        if x[i][0:-1][a:a+4] == 'spec':\n",
    "            filename = x[i][0:-1][a:]\n",
    "            if not os.path.exists(filename) :\n",
    "\n",
    "                data = requests.get( url_lamost, stream=True)\n",
    "                with open(data_filename, 'wb') as fdd:\n",
    "                    for chunk in data.iter_content(chunk_size=1024): \n",
    "                        if chunk: # filter out keep-alive new chunks\n",
    "                            fdd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptf1 = pyfits.open('spec-56270-HD110609N070823M01_sp07-075.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ra = ptf1[0].header['RA']\n",
    "dec = ptf1[0].header['Dec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167.929682, 5.417154)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra, dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_target = pd.read_csv('epic_ha663_normal-hasub_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201802777\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_target['RA'])):\n",
    "    if ra == all_target['RA'][i] and dec == all_target['Dec'][i]:\n",
    "        print all_target['EPIC'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201802777\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_target['RA'])):\n",
    "    if m.sqrt(((ra-all_target['RA'][i]))**2+((dec-all_target['Dec'][i]))**2) < 2./3600:\n",
    "        print all_target['EPIC'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"D:\\kepler_ptf_research\\k2_c1\\python_LAMOST_data_k2c1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\kepler_ptf_research\\\\k2_c1\\\\python_LAMOST_data_k2c1'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = gzip.open('spec-56270-HD110609N070823M01_sp07-075.fits.gz', 'rb')\n",
    "file_content = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#column_names = ['EPIC','RA','Dec','GZ','FITS']  \n",
    "\n",
    "#df = pd.DataFrame([ ], columns = column_names)\n",
    "#df.to_csv('lamost_data1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def append_csv(x1, x2, x3, x4, x5):\n",
    "\n",
    "    df = pd.read_csv('lamost_data1.csv')\n",
    "    column_names =  ['EPIC','RA','Dec','GZ','FITS']\n",
    "    \n",
    "    if x4 not in list(df['GZ'])  :\n",
    "    \n",
    "    \n",
    "        df2 = pd.DataFrame([[x1, x2, x3, x4, x5]], columns = column_names)\n",
    "\n",
    "        df_add = df.append(df2)\n",
    "\n",
    "        df_add.to_csv('lamost_data1.csv', index = False)\n",
    "    #else:\n",
    "        #print x3,'already exists'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('lamost_data1.csv')\n",
    "\n",
    "def lamost_data_crawler(url_list):\n",
    "\n",
    "    for i, v in enumerate(url_list):\n",
    "        url_lamost = v[0:-1]\n",
    "\n",
    "        data_filename = url_lamost.split('/')[-1]\n",
    "\n",
    "        if data_filename not in list(df['GZ']) or data_filename not in list(df['PNG']):\n",
    "\n",
    "            if not os.path.exists(data_filename) :\n",
    "\n",
    "                data = requests.get( url_lamost, stream=True)\n",
    "                with open(data_filename, 'wb') as fdd:\n",
    "                    for chunk in data.iter_content(chunk_size=1024): \n",
    "                        if chunk: # filter out keep-alive new chunks\n",
    "                            fdd.write(chunk)\n",
    "\n",
    "            if '.gz' in data_filename and not os.path.exists(data_filename[:-3]):\n",
    "                patoolib.extract_archive(data_filename)\n",
    "\n",
    "                ptf1 = pyfits.open(data_filename[:-3])\n",
    "                ra = ptf1[0].header['RA']\n",
    "                dec = ptf1[0].header['Dec']\n",
    "                \n",
    "                for i in range(len(all_target['RA'])):\n",
    "                    if m.sqrt(((ra-all_target['RA'][i]))**2+((dec-all_target['Dec'][i]))**2) < 2./3600:\n",
    "                        \n",
    "                        append_csv( all_target['EPIC'][i], all_target['RA'][i], all_target['Dec'][i], data_filename, data_filename[:-3])\n",
    "                        \n",
    "                        if not os.path.exists('ktwo'+str(all_target['EPIC'][i])+'_C1'):\n",
    "                            os.mkdir('ktwo'+str( all_target['EPIC'][i])+'_C1')\n",
    "                            \n",
    "                        ptf1.close()\n",
    "                        \n",
    "                        if not os.path.exists('D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'+str(all_target['EPIC'][i])+'_C1/'+data_filename[:-3]):\n",
    "                            os.rename(data_filename[:-3], 'D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'+str(all_target['EPIC'][i])+'_C1/'+data_filename[:-3])\n",
    "                        if not os.path.exists('D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'+str(all_target['EPIC'][i])+'_C1/'+filename):\n",
    "                            os.rename(data_filename, 'D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'+str(all_target['EPIC'][i])+'_C1/'+data_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patool: Extracting spec-56283-HD111915N004324B01_sp04-083.fits.gz ...\n",
      "patool: ... spec-56283-HD111915N004324B01_sp04-083.fits.gz extracted to `spec-56283-HD111915N004324B01_sp04-083.fits'.\n"
     ]
    },
    {
     "ename": "WindowsError",
     "evalue": "[Error 183] 當檔案已存在時，無法建立該檔案。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWindowsError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-192-1d3b0941a8ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlamost_data_crawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-186-2c8893e9ef55>\u001b[0m in \u001b[0;36mlamost_data_crawler\u001b[1;34m(url_list)\u001b[0m\n\u001b[0;32m     38\u001b[0m                             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_filename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'EPIC'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_C1/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdata_filename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'EPIC'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_C1/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'EPIC'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_C1/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdata_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mWindowsError\u001b[0m: [Error 183] 當檔案已存在時，無法建立該檔案。"
     ]
    }
   ],
   "source": [
    "#lamost_data_crawler(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if '.gz' in 'spec-56270-HD110609N070823M01_sp07-075.fits.gz':\n",
    "    print 'yes'\n",
    "else:\n",
    "    print 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spec-56270-HD110609N070823M01_sp07-075.fits.gz'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0:-1].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_gz_list = []\n",
    "\n",
    "for i in range(len(x)):\n",
    "    if '.gz' in x[i]:\n",
    "        x_gz_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_gz_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('lamost_data1.csv')\n",
    "\n",
    "def lamost_data_crawler(url):\n",
    "    \n",
    "\n",
    "    url_lamost = url[0:-1]\n",
    "    #url_lamost = url\n",
    "    data_filename = url_lamost.split('/')[-1]\n",
    "    #data_filename = url.split('/')[-1]\n",
    "    \n",
    "    if data_filename not in list(df['GZ']) and '.gz' in data_filename:\n",
    "\n",
    "        if not os.path.exists(data_filename):\n",
    "\n",
    "            data = requests.get( url_lamost, stream=True)\n",
    "            with open(data_filename, 'wb') as fdd:\n",
    "                for chunk in data.iter_content(chunk_size=1024): \n",
    "                    if chunk: # filter out keep-alive new chunks\n",
    "                        fdd.write(chunk)\n",
    "\n",
    "        if '.gz' in data_filename and not os.path.exists(data_filename[:-3]):\n",
    "            patoolib.extract_archive(data_filename)\n",
    "\n",
    "            ptf1 = pyfits.open(data_filename[:-3])\n",
    "            ra = ptf1[0].header['RA']\n",
    "            dec = ptf1[0].header['Dec']\n",
    "\n",
    "            for i in range(len(all_target['RA'])):\n",
    "                if m.sqrt(((ra-all_target['RA'][i]))**2+((dec-all_target['Dec'][i]))**2) < 2./3600:\n",
    "\n",
    "                    append_csv( all_target['EPIC'][i], all_target['RA'][i], all_target['Dec'][i], data_filename, data_filename[:-3])\n",
    "\n",
    "                    if not os.path.exists('ktwo'+str(all_target['EPIC'][i])+'_C1'):\n",
    "                        os.mkdir('ktwo'+str( all_target['EPIC'][i])+'_C1')\n",
    "\n",
    "                    ptf1.close()\n",
    "\n",
    "                    if not os.path.exists('D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'+str(all_target['EPIC'][i])+'_C1/'+data_filename[:-3]):\n",
    "                        os.rename(data_filename[:-3], 'D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'+str(all_target['EPIC'][i])+'_C1/'+data_filename[:-3])\n",
    "                    if not os.path.exists('D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'+str(all_target['EPIC'][i])+'_C1/'+filename):\n",
    "                        os.rename(data_filename, 'D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'+str(all_target['EPIC'][i])+'_C1/'+data_filename)\n",
    "    \n",
    "    elif '.png' in data_filename:\n",
    "        if not os.path.exists(data_filename):\n",
    "\n",
    "            data = requests.get( url_lamost, stream=True)\n",
    "            with open(data_filename, 'wb') as fdd:\n",
    "                for chunk in data.iter_content(chunk_size=1024): \n",
    "                    if chunk: # filter out keep-alive new chunks\n",
    "                        fdd.write(chunk)\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patool: Extracting spec-56270-HD110609N070823M01_sp07-075.fits.gz ...\n",
      "patool: ... spec-56270-HD110609N070823M01_sp07-075.fits.gz extracted to `spec-56270-HD110609N070823M01_sp07-075.fits'.\n",
      "patool: Extracting spec-56283-HD111915N004324B01_sp04-083.fits.gz ...\n",
      "patool: ... spec-56283-HD111915N004324B01_sp04-083.fits.gz extracted to `spec-56283-HD111915N004324B01_sp04-083.fits'.\n",
      "patool: Extracting spec-56283-HD111915N004324B01_sp15-144.fits.gz ...\n",
      "patool: ... spec-56283-HD111915N004324B01_sp15-144.fits.gz extracted to `spec-56283-HD111915N004324B01_sp15-144.fits'.\n",
      "patool: Extracting spec-56283-HD111915N004324M01_sp09-084.fits.gz ...\n",
      "patool: ... spec-56283-HD111915N004324M01_sp09-084.fits.gz extracted to `spec-56283-HD111915N004324M01_sp09-084.fits'.\n",
      "patool: Extracting spec-56283-HD111915N004324M01_sp11-049.fits.gz ...\n",
      "patool: ... spec-56283-HD111915N004324M01_sp11-049.fits.gz extracted to `spec-56283-HD111915N004324M01_sp11-049.fits'.\n",
      "patool: Extracting spec-56283-HD111915N004324M01_sp11-052.fits.gz ...\n",
      "patool: ... spec-56283-HD111915N004324M01_sp11-052.fits.gz extracted to `spec-56283-HD111915N004324M01_sp11-052.fits'.\n",
      "patool: Extracting spec-56283-HD111915N004324M01_sp12-097.fits.gz ...\n",
      "patool: ... spec-56283-HD111915N004324M01_sp12-097.fits.gz extracted to `spec-56283-HD111915N004324M01_sp12-097.fits'.\n",
      "patool: Extracting spec-56284-HD114225N022143B01_sp06-094.fits.gz ...\n",
      "patool: ... spec-56284-HD114225N022143B01_sp06-094.fits.gz extracted to `spec-56284-HD114225N022143B01_sp06-094.fits'.\n",
      "patool: Extracting spec-56284-HD114225N022143B01_sp09-064.fits.gz ...\n",
      "patool: ... spec-56284-HD114225N022143B01_sp09-064.fits.gz extracted to `spec-56284-HD114225N022143B01_sp09-064.fits'.\n",
      "patool: Extracting spec-56284-HD114225N022143B01_sp12-067.fits.gz ...\n",
      "patool: ... spec-56284-HD114225N022143B01_sp12-067.fits.gz extracted to `spec-56284-HD114225N022143B01_sp12-067.fits'.\n",
      "patool: Extracting spec-56286-HD114225N022143B_sp02-001.fits.gz ...\n",
      "patool: ... spec-56286-HD114225N022143B_sp02-001.fits.gz extracted to `spec-56286-HD114225N022143B_sp02-001.fits'.\n",
      "patool: Extracting spec-56286-HD114225N022143B_sp02-019.fits.gz ...\n",
      "patool: ... spec-56286-HD114225N022143B_sp02-019.fits.gz extracted to `spec-56286-HD114225N022143B_sp02-019.fits'.\n",
      "patool: Extracting spec-56286-HD114225N022143B_sp05-187.fits.gz ...\n",
      "patool: ... spec-56286-HD114225N022143B_sp05-187.fits.gz extracted to `spec-56286-HD114225N022143B_sp05-187.fits'.\n",
      "patool: Extracting spec-56286-HD114225N022143B_sp06-094.fits.gz ...\n",
      "patool: ... spec-56286-HD114225N022143B_sp06-094.fits.gz extracted to `spec-56286-HD114225N022143B_sp06-094.fits'.\n",
      "patool: Extracting spec-56286-HD114225N022143B_sp12-046.fits.gz ...\n",
      "patool: ... spec-56286-HD114225N022143B_sp12-046.fits.gz extracted to `spec-56286-HD114225N022143B_sp12-046.fits'.\n",
      "patool: Extracting spec-56286-HD114225N022143M_sp03-131.fits.gz ...\n",
      "patool: ... spec-56286-HD114225N022143M_sp03-131.fits.gz extracted to `spec-56286-HD114225N022143M_sp03-131.fits'.\n",
      "patool: Extracting spec-56286-HD114225N022143M_sp05-176.fits.gz ...\n",
      "patool: ... spec-56286-HD114225N022143M_sp05-176.fits.gz extracted to `spec-56286-HD114225N022143M_sp05-176.fits'.\n",
      "patool: Extracting spec-56286-HD114225N022143M_sp06-094.fits.gz ...\n",
      "patool: ... spec-56286-HD114225N022143M_sp06-094.fits.gz extracted to `spec-56286-HD114225N022143M_sp06-094.fits'.\n",
      "patool: Extracting spec-56286-HD114225N022143M_sp12-046.fits.gz ...\n",
      "patool: ... spec-56286-HD114225N022143M_sp12-046.fits.gz extracted to `spec-56286-HD114225N022143M_sp12-046.fits'.\n",
      "patool: Extracting spec-56287-HD112126N063805B01_sp02-092.fits.gz ...\n",
      "patool: ... spec-56287-HD112126N063805B01_sp02-092.fits.gz extracted to `spec-56287-HD112126N063805B01_sp02-092.fits'.\n",
      "patool: Extracting spec-56287-HD112126N063805B01_sp07-198.fits.gz ...\n",
      "patool: ... spec-56287-HD112126N063805B01_sp07-198.fits.gz extracted to `spec-56287-HD112126N063805B01_sp07-198.fits'.\n",
      "patool: Extracting spec-56287-HD112126N063805B01_sp08-162.fits.gz ...\n",
      "patool: ... spec-56287-HD112126N063805B01_sp08-162.fits.gz extracted to `spec-56287-HD112126N063805B01_sp08-162.fits'.\n",
      "patool: Extracting spec-56287-HD112126N063805M01_sp02-092.fits.gz ...\n",
      "patool: ... spec-56287-HD112126N063805M01_sp02-092.fits.gz extracted to `spec-56287-HD112126N063805M01_sp02-092.fits'.\n",
      "patool: Extracting spec-56287-HD112126N063805M01_sp07-041.fits.gz ...\n",
      "patool: ... spec-56287-HD112126N063805M01_sp07-041.fits.gz extracted to `spec-56287-HD112126N063805M01_sp07-041.fits'.\n",
      "patool: Extracting spec-56287-HD112126N063805M01_sp07-198.fits.gz ...\n",
      "patool: ... spec-56287-HD112126N063805M01_sp07-198.fits.gz extracted to `spec-56287-HD112126N063805M01_sp07-198.fits'.\n",
      "patool: Extracting spec-56287-HD112126N063805M01_sp08-162.fits.gz ...\n",
      "patool: ... spec-56287-HD112126N063805M01_sp08-162.fits.gz extracted to `spec-56287-HD112126N063805M01_sp08-162.fits'.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x)):\n",
    "    lamost_data_crawler(x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    lamost_data_crawler(x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('lamost_data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is in list\n"
     ]
    }
   ],
   "source": [
    "if 'spec-56283-HD111915N004324B01_sp04-083.fits.gz' not in list(df['GZ']):\n",
    "    print 'no'\n",
    "else:\n",
    "    print 'It is in list'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
