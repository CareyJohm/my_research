{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "import pyfits, mechanize, bz2, os\n",
    "from matplotlib import pyplot as pl\n",
    "from numpy import *\n",
    "from scipy.optimize import leastsq\n",
    "import astropy.stats as sta\n",
    "from scipy import ndimage\n",
    "from glob import glob\n",
    "import pandas as pd  #為了讀csv檔。csv檔是以逗點分割的檔案。\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from astropy.io import ascii\n",
    "import concurrent.futures\n",
    "from time import sleep\n",
    "from astropy.table import Table\n",
    "import math as m\n",
    "import gzip\n",
    "import patoolib\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('url.txt', 'r')\n",
    "x = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://dr2.lamost.org/sas/fits/HD110609N070823M01/spec-56270-HD110609N070823M01_sp07-075.fits.gz'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[5][0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lamo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[5][0:-1][11:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for a in range(len(x[0][0:-1])):\n",
    "    if x[0][0:-1][a:a+4] == 'spec':\n",
    "        filename = x[0][0:-1][a:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spec-56270-HD110609N070823M01_sp07-075.fits.gz'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spec-56270-HD110609N070823M01_sp07-075.fits'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    \n",
    "    url_lamost = x[i][0:-1]\n",
    "\n",
    "    data_filename = url_lamost.split('/')[-1]\n",
    "    \n",
    "    \n",
    "    for a in range(len(x[i][0:-1])):\n",
    "        if x[i][0:-1][a:a+4] == 'spec':\n",
    "            filename = x[i][0:-1][a:]\n",
    "            if not os.path.exists(filename) :\n",
    "\n",
    "                data = requests.get( url_lamost, stream=True)\n",
    "                with open(data_filename, 'wb') as fdd:\n",
    "                    for chunk in data.iter_content(chunk_size=1024): \n",
    "                        if chunk: # filter out keep-alive new chunks\n",
    "                            fdd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptf1 = pyfits.open('spec-56270-HD110609N070823M01_sp07-075.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ra = ptf1[0].header['RA']\n",
    "dec = ptf1[0].header['Dec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167.929682, 5.417154)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra, dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_target = pd.read_csv('epic_ha663_normal-hasub_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201802777\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_target['RA'])):\n",
    "    if ra == all_target['RA'][i] and dec == all_target['Dec'][i]:\n",
    "        print all_target['EPIC'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201802777\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_target['RA'])):\n",
    "    if m.sqrt(((ra-all_target['RA'][i]))**2+((dec-all_target['Dec'][i]))**2) < 2./3600:\n",
    "        print all_target['EPIC'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"D:\\kepler_ptf_research\\k2_c1\\python_LAMOST_data_k2c1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\kepler_ptf_research\\\\k2_c1\\\\python_LAMOST_data_k2c1'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#column_names = ['EPIC','RA','Dec','GZ','FITS']  \n",
    "\n",
    "#df = pd.DataFrame([ ], columns = column_names)\n",
    "#df.to_csv('lamost_data1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def append_csv(x1, x2, x3, x4, x5):\n",
    "\n",
    "    df = pd.read_csv('lamost_data1.csv')\n",
    "    column_names =  ['EPIC','RA','Dec','GZ','FITS']\n",
    "    \n",
    "    if x4 not in list(df['GZ'])  :\n",
    "    \n",
    "    \n",
    "        df2 = pd.DataFrame([[x1, x2, x3, x4, x5]], columns = column_names)\n",
    "\n",
    "        df_add = df.append(df2)\n",
    "\n",
    "        df_add.to_csv('lamost_data1.csv', index = False)\n",
    "    #else:\n",
    "        #print x3,'already exists'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#不是最新的\n",
    "\n",
    "df = pd.read_csv('lamost_data1.csv')\n",
    "\n",
    "def lamost_data_crawler(url_list):\n",
    "\n",
    "    for i, v in enumerate(url_list):\n",
    "        url_lamost = v[0:-1]\n",
    "\n",
    "        data_filename = url_lamost.split('/')[-1]\n",
    "\n",
    "        if data_filename not in list(df['GZ']) or data_filename not in list(df['PNG']):\n",
    "\n",
    "            if not os.path.exists(data_filename) :\n",
    "\n",
    "                data = requests.get( url_lamost, stream=True)\n",
    "                with open(data_filename, 'wb') as fdd:\n",
    "                    for chunk in data.iter_content(chunk_size=1024): \n",
    "                        if chunk: # filter out keep-alive new chunks\n",
    "                            fdd.write(chunk)\n",
    "\n",
    "            if '.gz' in data_filename and not os.path.exists(data_filename[:-3]):\n",
    "                patoolib.extract_archive(data_filename)\n",
    "\n",
    "                ptf1 = pyfits.open(data_filename[:-3])\n",
    "                ra = ptf1[0].header['RA']\n",
    "                dec = ptf1[0].header['Dec']\n",
    "                \n",
    "                for i in range(len(all_target['RA'])):\n",
    "                    if m.sqrt(((ra-all_target['RA'][i]))**2+((dec-all_target['Dec'][i]))**2) < 2./3600:\n",
    "                        \n",
    "                        append_csv( all_target['EPIC'][i], all_target['RA'][i], all_target['Dec'][i], data_filename, data_filename[:-3])\n",
    "                        \n",
    "                        if not os.path.exists('ktwo'+str(all_target['EPIC'][i])+'_C1'):\n",
    "                            os.mkdir('ktwo'+str( all_target['EPIC'][i])+'_C1')\n",
    "                            \n",
    "                        ptf1.close()\n",
    "                        \n",
    "                        if not os.path.exists('D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'+str(all_target['EPIC'][i])+'_C1/'+data_filename[:-3]):\n",
    "                            os.rename(data_filename[:-3], 'D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'+str(all_target['EPIC'][i])+'_C1/'+data_filename[:-3])\n",
    "                        if not os.path.exists('D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'+str(all_target['EPIC'][i])+'_C1/'+filename):\n",
    "                            os.rename(data_filename, 'D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'+str(all_target['EPIC'][i])+'_C1/'+data_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patool: Extracting spec-56283-HD111915N004324B01_sp04-083.fits.gz ...\n",
      "patool: ... spec-56283-HD111915N004324B01_sp04-083.fits.gz extracted to `spec-56283-HD111915N004324B01_sp04-083.fits'.\n"
     ]
    },
    {
     "ename": "WindowsError",
     "evalue": "[Error 183] 當檔案已存在時，無法建立該檔案。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWindowsError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-192-1d3b0941a8ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlamost_data_crawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-186-2c8893e9ef55>\u001b[0m in \u001b[0;36mlamost_data_crawler\u001b[1;34m(url_list)\u001b[0m\n\u001b[0;32m     38\u001b[0m                             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_filename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'EPIC'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_C1/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdata_filename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'EPIC'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_C1/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'EPIC'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_C1/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdata_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mWindowsError\u001b[0m: [Error 183] 當檔案已存在時，無法建立該檔案。"
     ]
    }
   ],
   "source": [
    "#lamost_data_crawler(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if '.gz' in 'spec-56270-HD110609N070823M01_sp07-075.fits.gz':\n",
    "    print 'yes'\n",
    "else:\n",
    "    print 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spec-56270-HD110609N070823M01_sp07-075.fits.gz'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0:-1].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_gz_list = []\n",
    "\n",
    "for i in range(len(x)):\n",
    "    if '.gz' in x[i]:\n",
    "        x_gz_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_gz_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#最新版本\n",
    "\n",
    "df = pd.read_csv('lamost_data1.csv')\n",
    "\n",
    "def lamost_data_crawler(url):\n",
    "    \n",
    "\n",
    "    url_lamost = url[0:-1]\n",
    "    #url_lamost = url\n",
    "    data_filename = url_lamost.split('/')[-1]\n",
    "    #data_filename = url.split('/')[-1]\n",
    "    \n",
    "    if data_filename not in list(df['GZ']) and '.gz' in data_filename:\n",
    "\n",
    "        if not os.path.exists(data_filename):\n",
    "\n",
    "            data = requests.get( url_lamost, stream=True)\n",
    "            with open(data_filename, 'wb') as fdd:\n",
    "                for chunk in data.iter_content(chunk_size=1024): \n",
    "                    if chunk: # filter out keep-alive new chunks\n",
    "                        fdd.write(chunk)\n",
    "\n",
    "        if '.gz' in data_filename and not os.path.exists(data_filename[:-3]):\n",
    "            patoolib.extract_archive(data_filename)\n",
    "\n",
    "            ptf1 = pyfits.open(data_filename[:-3])\n",
    "            ra = ptf1[0].header['RA']\n",
    "            dec = ptf1[0].header['Dec']\n",
    "\n",
    "            for i in range(len(all_target['RA'])):\n",
    "                if m.sqrt(((ra-all_target['RA'][i]))**2+((dec-all_target['Dec'][i]))**2) < 2./3600:\n",
    "\n",
    "                    append_csv( all_target['EPIC'][i], all_target['RA'][i], all_target['Dec'][i], data_filename, data_filename[:-3])\n",
    "\n",
    "                    if not os.path.exists('ktwo'+str(all_target['EPIC'][i])+'_C1'):\n",
    "                        os.mkdir('ktwo'+str( all_target['EPIC'][i])+'_C1')\n",
    "\n",
    "                    ptf1.close()\n",
    "\n",
    "                    if not os.path.exists('D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'+str(all_target['EPIC'][i])+'_C1/'+data_filename[:-3]):\n",
    "                        os.rename(data_filename[:-3], 'D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'+str(all_target['EPIC'][i])+'_C1/'+data_filename[:-3])\n",
    "                    if not os.path.exists('D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'+str(all_target['EPIC'][i])+'_C1/'+filename):\n",
    "                        os.rename(data_filename, 'D:/kepler_ptf_research/k2_c1/python_LAMOST_data_k2c1/ktwo'+str(all_target['EPIC'][i])+'_C1/'+data_filename)\n",
    "    \n",
    "    elif '.png' in data_filename:\n",
    "        if not os.path.exists(data_filename):\n",
    "\n",
    "            data = requests.get( url_lamost, stream=True)\n",
    "            with open(data_filename, 'wb') as fdd:\n",
    "                for chunk in data.iter_content(chunk_size=1024): \n",
    "                    if chunk: # filter out keep-alive new chunks\n",
    "                        fdd.write(chunk)\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    lamost_data_crawler(x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    lamost_data_crawler(x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('lamost_data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is in list\n"
     ]
    }
   ],
   "source": [
    "if 'spec-56283-HD111915N004324B01_sp04-083.fits.gz' not in list(df['GZ']):\n",
    "    print 'no'\n",
    "else:\n",
    "    print 'It is in list'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     spec-56270-HD110609N070823M01_sp07-075.fits\n",
       "1     spec-56283-HD111915N004324B01_sp04-083.fits\n",
       "2     spec-56283-HD111915N004324B01_sp15-144.fits\n",
       "3     spec-56283-HD111915N004324M01_sp09-084.fits\n",
       "4     spec-56283-HD111915N004324M01_sp11-049.fits\n",
       "5     spec-56283-HD111915N004324M01_sp11-052.fits\n",
       "6     spec-56283-HD111915N004324M01_sp12-097.fits\n",
       "7     spec-56284-HD114225N022143B01_sp06-094.fits\n",
       "8     spec-56284-HD114225N022143B01_sp09-064.fits\n",
       "9     spec-56284-HD114225N022143B01_sp12-067.fits\n",
       "10      spec-56286-HD114225N022143B_sp02-001.fits\n",
       "11      spec-56286-HD114225N022143B_sp02-019.fits\n",
       "12      spec-56286-HD114225N022143B_sp05-187.fits\n",
       "13      spec-56286-HD114225N022143B_sp06-094.fits\n",
       "14      spec-56286-HD114225N022143B_sp12-046.fits\n",
       "15      spec-56286-HD114225N022143M_sp03-131.fits\n",
       "16      spec-56286-HD114225N022143M_sp05-176.fits\n",
       "17      spec-56286-HD114225N022143M_sp06-094.fits\n",
       "18      spec-56286-HD114225N022143M_sp12-046.fits\n",
       "19    spec-56287-HD112126N063805B01_sp02-092.fits\n",
       "20    spec-56287-HD112126N063805B01_sp07-198.fits\n",
       "21    spec-56287-HD112126N063805B01_sp08-162.fits\n",
       "22    spec-56287-HD112126N063805M01_sp02-092.fits\n",
       "23    spec-56287-HD112126N063805M01_sp07-041.fits\n",
       "24    spec-56287-HD112126N063805M01_sp07-198.fits\n",
       "25    spec-56287-HD112126N063805M01_sp08-162.fits\n",
       "Name: FITS, dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['FITS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spec-56270-HD110609N070823M01_sp07-075.fits'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['FITS'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sp15-144'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['FITS'][2][-13:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sp11-052'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['FITS'][5][-13:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spec-56270-HD110609N070823M01_sp07-075.fits\n",
      "spec-56283-HD111915N004324B01_sp04-083.fits\n",
      "spec-56283-HD111915N004324B01_sp15-144.fits\n",
      "spec-56283-HD111915N004324M01_sp09-084.fits\n",
      "spec-56283-HD111915N004324M01_sp11-049.fits\n",
      "spec-56283-HD111915N004324M01_sp11-052.fits\n",
      "spec-56283-HD111915N004324M01_sp12-097.fits\n",
      "spec-56284-HD114225N022143B01_sp06-094.fits\n",
      "spec-56284-HD114225N022143B01_sp09-064.fits\n",
      "spec-56284-HD114225N022143B01_sp12-067.fits\n",
      "spec-56286-HD114225N022143B_sp02-001.fits\n",
      "spec-56286-HD114225N022143B_sp02-019.fits\n",
      "spec-56286-HD114225N022143B_sp05-187.fits\n",
      "spec-56286-HD114225N022143B_sp06-094.fits\n",
      "spec-56286-HD114225N022143B_sp12-046.fits\n",
      "spec-56286-HD114225N022143M_sp03-131.fits\n",
      "spec-56286-HD114225N022143M_sp05-176.fits\n",
      "spec-56286-HD114225N022143M_sp06-094.fits\n",
      "spec-56286-HD114225N022143M_sp12-046.fits\n",
      "spec-56287-HD112126N063805B01_sp02-092.fits\n",
      "spec-56287-HD112126N063805B01_sp07-198.fits\n",
      "spec-56287-HD112126N063805B01_sp08-162.fits\n",
      "spec-56287-HD112126N063805M01_sp02-092.fits\n",
      "spec-56287-HD112126N063805M01_sp07-041.fits\n",
      "spec-56287-HD112126N063805M01_sp07-198.fits\n",
      "spec-56287-HD112126N063805M01_sp08-162.fits\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df['FITS'])):\n",
    "    print df['FITS'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "png_list = glob.glob('*png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "png_r_list = []\n",
    "epic_r_list = []\n",
    "\n",
    "for i in range(len(png_list)):\n",
    "    if 'sp02-019' in png_list[i]:\n",
    "        png_r_list.append(png_list[i])\n",
    "        epic_r_list.append(df['EPIC'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spec-56286-HD114225N022143B_sp02-019.png']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "png_r_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'201493524'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(int(epic_r_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mv_png(fits):\n",
    "    png_r_list = []\n",
    "    epic_r_list = []\n",
    "    for i in range(len(png_list)):\n",
    "        if fits[-13:-5] in png_list[i]:\n",
    "            png_r_list.append(png_list[i])\n",
    "            epic_r_list.append(df['EPIC'][i])\n",
    "            \n",
    "    os.chdir('ktwo'+str(int(epic_r_list[0]))+'_C1/')\n",
    "    for a in range(len(png_r_list)):\n",
    "        if not os.path.exists(png_r_list[a]): \n",
    "            os.chdir('D:\\kepler_ptf_research\\k2_c1\\python_LAMOST_data_k2c1')\n",
    "            \n",
    "        else:\n",
    "            os.chdir('D:\\kepler_ptf_research\\k2_c1\\python_LAMOST_data_k2c1')\n",
    "            \n",
    "        if os.path.exists(png_r_list[a]):\n",
    "            os.rename(png_r_list[a], 'ktwo'+str(int(epic_r_list[0]))+'_C1/'+png_r_list[a])\n",
    "        #else:\n",
    "            #os.chdir('..')\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for s in range(len(df['FITS'])):\n",
    "    mv_png(df['FITS'][s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\kepler_ptf_research\\\\k2_c1\\\\python_LAMOST_data_k2c1'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('D:\\kepler_ptf_research\\k2_c1\\python_LAMOST_data_k2c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n",
      "p\n",
      "e\n",
      "c\n",
      "-\n",
      "5\n",
      "6\n",
      "2\n",
      "7\n",
      "0\n",
      "-\n",
      "H\n",
      "D\n",
      "1\n",
      "1\n",
      "0\n",
      "6\n",
      "0\n",
      "9\n",
      "N\n",
      "0\n",
      "7\n",
      "0\n",
      "8\n",
      "2\n",
      "3\n",
      "M\n",
      "0\n",
      "1\n",
      "_\n",
      "s\n",
      "p\n",
      "0\n",
      "7\n",
      "-\n",
      "0\n",
      "7\n",
      "5\n",
      ".\n",
      "f\n",
      "i\n",
      "t\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "for a in df['FITS'][0]:\n",
    "    print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
